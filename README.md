This project includes a comprehensive post-training evaluation pipeline designed to analyze the behavior of a YOLO-based object detection model beyond standard accuracy metrics. The evaluation was conducted using an IoU threshold of 0.5 on a total of 1,817 CT images, and all results were automatically generated after model inference.

From the overall detection statistics, the dataset contains 327 ground truth objects, while the model produced 9,693 predicted bounding boxes. Among these predictions, 317 were classified as true positives, achieving a high average IoU of 0.8099, which indicates that the model is capable of accurately localizing the target structure when detections are correct. However, the number of false positives reached 9,376, greatly exceeding the number of true objects, while 10 false negatives and 140 duplicate detections were also observed. These results reveal that although localization quality is strong for correct detections, the model suffers from severe over-detection.

Further analysis of the confidence score distribution shows that a large proportion of false positive detections are assigned moderate to high confidence scores. This behavior suggests that the model is not merely producing low-confidence noise, but is instead overconfident when predicting background regions. As a consequence, the current confidence threshold is insufficient to effectively filter unreliable predictions, directly contributing to the excessive false positive rate observed in the evaluation results.

The IoU distribution analysis reveals a bimodal pattern. Correct detections are primarily clustered at high IoU values, confirming the modelâ€™s ability to precisely localize the target object under favorable conditions. In contrast, a substantial number of predictions fall into low-IoU ranges, indicating inaccurate localization or detections on non-target anatomical structures. This discrepancy reflects instability in bounding box regression, particularly when the model encounters difficult or ambiguous samples.

An object size analysis further highlights the challenges of this task. The aortic valve occupies a very small region within CT slices, making detection highly sensitive to input resolution, anchor configuration, and feature scale selection. The results show that small objects exhibit significantly lower detection reliability. To compensate for uncertainty, the model frequently generates larger bounding boxes, which reduces IoU and simultaneously increases the number of false positives. These findings confirm that this task is fundamentally a small-object detection problem, for which standard YOLO configurations are not fully optimized.

To better understand the spatial characteristics of incorrect predictions, a false positive heatmap was generated. The heatmap reveals that false positives are not randomly distributed, but instead cluster in specific regions of the image. These regions often correspond to anatomical structures with similar intensity or shape to the target, such as blood vessels or calcified tissue. This pattern indicates systematic confusion between the target and visually similar structures, rather than random prediction noise, and provides valuable guidance for future improvements in data preprocessing and model design.

In addition to aggregated metrics, the evaluation pipeline outputs a list of high-risk images, including samples with excessive false positives, images containing false negatives, and images with multiple duplicate detections. These images are particularly useful for manual inspection and error diagnosis, as they expose failure cases that cannot be fully understood through numerical metrics alone.

In summary, the evaluation results demonstrate that the model has strong localization capability when predictions are correct, but is severely limited by an extremely high false positive rate. The primary causes of this issue include overconfident background predictions, the small scale of the target object, and the complexity of surrounding anatomical structures. Moreover, the current confidence threshold and non-maximum suppression settings are insufficient to suppress unreliable detections. These findings explain the discrepancy between visually plausible results and low leaderboard performance, and they emphasize the need for targeted improvements rather than blind retraining.

Based on these observations, several directions for future improvement are proposed. These include increasing the confidence threshold and tuning non-maximum suppression parameters, redesigning anchor sizes specifically for small-object detection, increasing input resolution or incorporating multi-scale feature fusion, applying medical-image-specific data augmentation techniques, and exploring segmentation-assisted or hybrid detection approaches to better constrain the prediction space.
